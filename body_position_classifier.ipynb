{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Directory containing your CSV files\n",
    "csv_directory = 'Data'\n",
    "\n",
    "# List to hold each dataframe\n",
    "dataframes = []\n",
    "\n",
    "#variables\n",
    "excercise_name = \"squat\"\n",
    "camera_position = \"side\"\n",
    "\n",
    "# Iterate through each CSV file in the directory\n",
    "for filename in os.listdir(csv_directory):\n",
    "    filename = filename.lower().strip()\n",
    "    if filename.endswith('.csv') and filename.startswith(f'{excercise_name}_{camera_position}'):\n",
    "        df = pd.read_csv(os.path.join(csv_directory, filename))\n",
    "        dataframes.append(df)\n",
    "\n",
    "# Concatenate all dataframes\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Combined Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming 'target_column' is the name of your target variable\n",
    "X = combined_df.drop('position', axis=1)\n",
    "y = combined_df['position']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Some Features (Hands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['nose_x', 'nose_y', 'leftEyeInner_x', 'leftEyeInner_y', 'leftEye_x',\n",
       "       'leftEye_y', 'leftEyeOuter_x', 'leftEyeOuter_y', 'rightEyeInner_x',\n",
       "       'rightEyeInner_y', 'rightEye_x', 'rightEye_y', 'rightEyeOuter_x',\n",
       "       'rightEyeOuter_y', 'leftEar_x', 'leftEar_y', 'rightEar_x', 'rightEar_y',\n",
       "       'leftMouth_x', 'leftMouth_y', 'rightMouth_x', 'rightMouth_y',\n",
       "       'leftShoulder_x', 'leftShoulder_y', 'rightShoulder_x',\n",
       "       'rightShoulder_y', 'leftHip_x', 'leftHip_y', 'rightHip_x', 'rightHip_y',\n",
       "       'leftKnee_x', 'leftKnee_y', 'rightKnee_x', 'rightKnee_y', 'leftAnkle_x',\n",
       "       'leftAnkle_y', 'rightAnkle_x', 'rightAnkle_y', 'leftHeel_x',\n",
       "       'leftHeel_y', 'rightHeel_x', 'rightHeel_y', 'leftFootIndex_x',\n",
       "       'leftFootIndex_y', 'rightFootIndex_x', 'rightFootIndex_y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings_to_search = ['Elbow', 'Wrist', 'Pinky', 'leftIndex', 'rightIndex', 'Thumb' , '_z']\n",
    "columns_to_drop = X.columns[X.columns.str.contains('|'.join(strings_to_search))]\n",
    "X = X.drop(columns=columns_to_drop)\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "means = scaler.mean_.tolist()\n",
    "stds = scaler.scale_.tolist()\n",
    "\n",
    "with open(f'means.json', 'w') as f:\n",
    "    json.dump(means, f)\n",
    "\n",
    "with open(f'stds.json', 'w') as f:\n",
    "    json.dump(stds, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Train a RandomForestClassifier\n",
    "# model = RandomForestClassifier()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate the model\n",
    "# y_pred = model.predict(X_test)\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# # Save the model using joblib\n",
    "# import joblib\n",
    "# joblib.dump(model, f'{excercise_name}_{camera_position}.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/keras/src/engine/data_adapter.py:1798: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  return t[start:end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 1s 2ms/step - loss: 0.1929 - accuracy: 0.9377 - val_loss: 0.1618 - val_accuracy: 0.9399\n",
      "Epoch 2/100\n",
      "246/246 [==============================] - 0s 861us/step - loss: 0.1434 - accuracy: 0.9518 - val_loss: 0.1480 - val_accuracy: 0.9429\n",
      "Epoch 3/100\n",
      "246/246 [==============================] - 0s 762us/step - loss: 0.1282 - accuracy: 0.9547 - val_loss: 0.1349 - val_accuracy: 0.9480\n",
      "Epoch 4/100\n",
      "246/246 [==============================] - 0s 707us/step - loss: 0.1221 - accuracy: 0.9546 - val_loss: 0.1304 - val_accuracy: 0.9485\n",
      "Epoch 5/100\n",
      "246/246 [==============================] - 0s 712us/step - loss: 0.1162 - accuracy: 0.9570 - val_loss: 0.1197 - val_accuracy: 0.9506\n",
      "Epoch 6/100\n",
      "246/246 [==============================] - 0s 692us/step - loss: 0.1107 - accuracy: 0.9573 - val_loss: 0.1279 - val_accuracy: 0.9501\n",
      "Epoch 7/100\n",
      "246/246 [==============================] - 0s 811us/step - loss: 0.1065 - accuracy: 0.9583 - val_loss: 0.1102 - val_accuracy: 0.9557\n",
      "Epoch 8/100\n",
      "246/246 [==============================] - 0s 701us/step - loss: 0.1038 - accuracy: 0.9588 - val_loss: 0.1223 - val_accuracy: 0.9501\n",
      "Epoch 9/100\n",
      "246/246 [==============================] - 0s 693us/step - loss: 0.1014 - accuracy: 0.9591 - val_loss: 0.1115 - val_accuracy: 0.9557\n",
      "Epoch 10/100\n",
      "246/246 [==============================] - 0s 690us/step - loss: 0.0972 - accuracy: 0.9618 - val_loss: 0.1059 - val_accuracy: 0.9587\n",
      "Epoch 11/100\n",
      "246/246 [==============================] - 0s 695us/step - loss: 0.0991 - accuracy: 0.9587 - val_loss: 0.1040 - val_accuracy: 0.9567\n",
      "Epoch 12/100\n",
      "246/246 [==============================] - 0s 681us/step - loss: 0.0939 - accuracy: 0.9614 - val_loss: 0.1123 - val_accuracy: 0.9551\n",
      "Epoch 13/100\n",
      "246/246 [==============================] - 0s 685us/step - loss: 0.0938 - accuracy: 0.9609 - val_loss: 0.1119 - val_accuracy: 0.9551\n",
      "Epoch 14/100\n",
      "246/246 [==============================] - 0s 774us/step - loss: 0.0891 - accuracy: 0.9643 - val_loss: 0.1138 - val_accuracy: 0.9531\n",
      "Epoch 15/100\n",
      "246/246 [==============================] - 0s 699us/step - loss: 0.0906 - accuracy: 0.9625 - val_loss: 0.1133 - val_accuracy: 0.9602\n",
      "Epoch 16/100\n",
      "246/246 [==============================] - 0s 734us/step - loss: 0.0874 - accuracy: 0.9648 - val_loss: 0.1127 - val_accuracy: 0.9541\n",
      "Epoch 17/100\n",
      "246/246 [==============================] - 0s 860us/step - loss: 0.0902 - accuracy: 0.9639 - val_loss: 0.1058 - val_accuracy: 0.9602\n",
      "Epoch 18/100\n",
      "246/246 [==============================] - 0s 882us/step - loss: 0.0838 - accuracy: 0.9648 - val_loss: 0.0985 - val_accuracy: 0.9623\n",
      "Epoch 19/100\n",
      "246/246 [==============================] - 0s 831us/step - loss: 0.0830 - accuracy: 0.9667 - val_loss: 0.1015 - val_accuracy: 0.9613\n",
      "Epoch 20/100\n",
      "246/246 [==============================] - 0s 842us/step - loss: 0.0809 - accuracy: 0.9657 - val_loss: 0.0981 - val_accuracy: 0.9602\n",
      "Epoch 21/100\n",
      "246/246 [==============================] - 0s 779us/step - loss: 0.0798 - accuracy: 0.9685 - val_loss: 0.1035 - val_accuracy: 0.9577\n",
      "Epoch 22/100\n",
      "246/246 [==============================] - 0s 700us/step - loss: 0.0774 - accuracy: 0.9692 - val_loss: 0.1003 - val_accuracy: 0.9608\n",
      "Epoch 23/100\n",
      "246/246 [==============================] - 0s 789us/step - loss: 0.0790 - accuracy: 0.9656 - val_loss: 0.1005 - val_accuracy: 0.9613\n",
      "Epoch 24/100\n",
      "246/246 [==============================] - 0s 852us/step - loss: 0.0757 - accuracy: 0.9666 - val_loss: 0.0997 - val_accuracy: 0.9638\n",
      "Epoch 25/100\n",
      "246/246 [==============================] - 0s 820us/step - loss: 0.0738 - accuracy: 0.9676 - val_loss: 0.0954 - val_accuracy: 0.9648\n",
      "Epoch 26/100\n",
      "246/246 [==============================] - 0s 757us/step - loss: 0.0744 - accuracy: 0.9706 - val_loss: 0.0951 - val_accuracy: 0.9664\n",
      "Epoch 27/100\n",
      "246/246 [==============================] - 0s 691us/step - loss: 0.0719 - accuracy: 0.9711 - val_loss: 0.1034 - val_accuracy: 0.9592\n",
      "Epoch 28/100\n",
      "246/246 [==============================] - 0s 722us/step - loss: 0.0683 - accuracy: 0.9694 - val_loss: 0.0902 - val_accuracy: 0.9633\n",
      "Epoch 29/100\n",
      "246/246 [==============================] - 0s 855us/step - loss: 0.0679 - accuracy: 0.9708 - val_loss: 0.1229 - val_accuracy: 0.9531\n",
      "Epoch 30/100\n",
      "246/246 [==============================] - 0s 675us/step - loss: 0.0696 - accuracy: 0.9722 - val_loss: 0.1021 - val_accuracy: 0.9613\n",
      "Epoch 31/100\n",
      "246/246 [==============================] - 0s 671us/step - loss: 0.0695 - accuracy: 0.9692 - val_loss: 0.0995 - val_accuracy: 0.9628\n",
      "Epoch 32/100\n",
      "246/246 [==============================] - 0s 672us/step - loss: 0.0641 - accuracy: 0.9727 - val_loss: 0.0928 - val_accuracy: 0.9623\n",
      "Epoch 33/100\n",
      "246/246 [==============================] - 0s 668us/step - loss: 0.0670 - accuracy: 0.9716 - val_loss: 0.0922 - val_accuracy: 0.9633\n",
      "Epoch 34/100\n",
      "246/246 [==============================] - 0s 667us/step - loss: 0.0638 - accuracy: 0.9741 - val_loss: 0.1033 - val_accuracy: 0.9562\n",
      "Epoch 35/100\n",
      "246/246 [==============================] - 0s 669us/step - loss: 0.0628 - accuracy: 0.9737 - val_loss: 0.0968 - val_accuracy: 0.9653\n",
      "Epoch 36/100\n",
      "246/246 [==============================] - 0s 668us/step - loss: 0.0615 - accuracy: 0.9741 - val_loss: 0.1010 - val_accuracy: 0.9557\n",
      "Epoch 37/100\n",
      "246/246 [==============================] - 0s 677us/step - loss: 0.0588 - accuracy: 0.9737 - val_loss: 0.0911 - val_accuracy: 0.9633\n",
      "Epoch 38/100\n",
      "246/246 [==============================] - 0s 673us/step - loss: 0.0580 - accuracy: 0.9743 - val_loss: 0.0948 - val_accuracy: 0.9664\n",
      "77/77 [==============================] - 0s 372us/step - loss: 0.0774 - accuracy: 0.9662\n",
      "Test accuracy: 0.9661501049995422\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "#Check model accuracy\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# Save the model\n",
    "# model.save(f'{excercise_name}_{camera_position}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/7z/111t5wv14k194p7f0gnjzmg00000gn/T/tmpw7x_9app/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/7z/111t5wv14k194p7f0gnjzmg00000gn/T/tmpw7x_9app/assets\n",
      "2024-06-10 23:26:02.316145: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2024-06-10 23:26:02.316161: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2024-06-10 23:26:02.316345: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /var/folders/7z/111t5wv14k194p7f0gnjzmg00000gn/T/tmpw7x_9app\n",
      "2024-06-10 23:26:02.316907: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2024-06-10 23:26:02.316912: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /var/folders/7z/111t5wv14k194p7f0gnjzmg00000gn/T/tmpw7x_9app\n",
      "2024-06-10 23:26:02.318864: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2024-06-10 23:26:02.347690: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /var/folders/7z/111t5wv14k194p7f0gnjzmg00000gn/T/tmpw7x_9app\n",
      "2024-06-10 23:26:02.356580: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 40227 microseconds.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model\n",
    "with open(f'model_{excercise_name}_{camera_position}.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
